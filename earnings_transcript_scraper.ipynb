{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kerry Zhang\n",
    "# 7/16/2023\n",
    "# Objective: Scrape earnings call transcripts from roic.ai\n",
    "# Notes: Earnings transcripts are divided by person with div class \"p-3 rounded-lg false\".\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from textblob import TextBlob\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Avoid throttling\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "chrome_options.add_argument(\"--headless\")  # Enable headless mode\n",
    "\n",
    "# Options\n",
    "\n",
    "INPUT_FILE = \"firms_full.csv\"\n",
    "LOG_FILE = \"scrape_log.csv\"\n",
    "OUTPUT_FOLDER = r\"D:\\finance_tools\\transcripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Firms: 3049\n",
      "Total Firm-Quarters: 100845\n",
      "\n",
      "\n",
      "REMAINING: \n",
      "Unique Firms: 3044\n",
      "Total Firm-Quarters: 100458\n"
     ]
    }
   ],
   "source": [
    "# Load tickers\n",
    "TICKER_DATA = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Require known fiscal quarter\n",
    "TICKER_DATA = TICKER_DATA[TICKER_DATA['fqtr'].notna()]\n",
    "\n",
    "# Require active\n",
    "TICKER_DATA = TICKER_DATA[TICKER_DATA['costat'] == \"A\"]\n",
    "\n",
    "# Require known market value > $1B\n",
    "TICKER_DATA = TICKER_DATA[TICKER_DATA['mkvaltq'].notna()]\n",
    "TICKER_DATA = TICKER_DATA[TICKER_DATA['mkvaltq'] > 1000]\n",
    "\n",
    "# Clean\n",
    "TICKER_DATA = TICKER_DATA[[\"tic\", 'fyearq', 'fqtr']].reset_index()\n",
    "TICKER_DATA['fqtr'] = TICKER_DATA['fqtr'].astype('int')\n",
    "\n",
    "print(\"Unique Firms: {}\".format(TICKER_DATA[\"tic\"].nunique()))\n",
    "print(\"Total Firm-Quarters: {}\".format(TICKER_DATA.shape[0]))\n",
    "\n",
    "# Remove already scraped\n",
    "if os.path.exists(LOG_FILE):\n",
    "    log_df = pd.read_csv(LOG_FILE)\n",
    "    TICKER_DATA = pd.merge(TICKER_DATA, log_df, how = \"left\", on = [\"tic\", 'fyearq', 'fqtr'])\n",
    "    TICKER_DATA = TICKER_DATA[TICKER_DATA[\"captured\"].isna()].reset_index()\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"REMAINING: \")\n",
    "    print(\"Unique Firms: {}\".format(TICKER_DATA[\"tic\"].nunique()))\n",
    "    print(\"Total Firm-Quarters: {}\".format(TICKER_DATA.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_to_disappear_and_return_new_text(object):\n",
    "    def __init__(self, locator, initial_text):\n",
    "        self.locator = locator\n",
    "        self.initial_text = initial_text\n",
    "\n",
    "    def __call__(self, driver):\n",
    "        try:\n",
    "            element = driver.find_element(*self.locator)\n",
    "            element_text = element.text\n",
    "            if self.initial_text not in element_text:\n",
    "                return element_text\n",
    "        except:\n",
    "            # If element is not found or initial text still present, return False to continue waiting.\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_transcript(url):\n",
    "    \n",
    "    # Set up Selenium WebDriver\n",
    "    driver = webdriver.Chrome(chrome_options)  \n",
    "    \n",
    "    # Navigate to the website\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait until element is loaded\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    locator = (By.CLASS_NAME, \"space-y-6\")\n",
    "    initial_text = \"Please wait for a while ...\"\n",
    "    \n",
    "    element = wait.until(text_to_disappear_and_return_new_text(locator, initial_text))\n",
    "        \n",
    "    # Extract Text\n",
    "    text = copy.deepcopy(element)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(earnings_transcript: str) -> list:\n",
    "\n",
    "    # WIP: Remove non-ASCII characters\n",
    "    earnings_transcript = earnings_transcript.replace(\"â\\x80\\x99\", \"'\").replace(\"â\\x80\\x98\", \"'\").replace(\"â\\x80\\x93\", \"'\")\n",
    "    \n",
    "    lst = earnings_transcript.splitlines()\n",
    "    \n",
    "    # Remove speaker abbreviations\n",
    "    #lst = [element for element in lst if len(element) > 1]\n",
    "    \n",
    "    return lst\n",
    "\n",
    "def parse_text(earnings_transcript: str) -> tuple:\n",
    "    \n",
    "    # Identify entities\n",
    "    doc = nlp(earnings_transcript)\n",
    "    \n",
    "    people = []\n",
    "    orgs = []\n",
    "    for token in doc:\n",
    "        if token.ent_type_ == \"PERSON\":\n",
    "            people.append(token.text)\n",
    "        if token.ent_type_ == \"ORG\":\n",
    "            orgs.append(token.text)\n",
    "            \n",
    "    return people, orgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(lst: list, year: int, quarter: int, people, orgs):\n",
    "    \n",
    "    def is_name(segment: str) -> bool:\n",
    "        if segment == \"Operator\" or segment in people:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "#         words_in_segment = segment.split()\n",
    "    \n",
    "#         if (\n",
    "#             segment == \"Operator\" or \n",
    "#             (\n",
    "#              len(words_in_segment) == 2 and \n",
    "#              words_in_segment[0].istitle() and \n",
    "#              words_in_segment[1].istitle() and\n",
    "#              \".\" not in segment\n",
    "#             )\n",
    "#            ):\n",
    "#             return True\n",
    "#         else:\n",
    "#             return False\n",
    "        \n",
    "    def get_sentiment(text):\n",
    "        analysis = TextBlob(text)\n",
    "        sentiment_score = analysis.sentiment.polarity\n",
    "        \n",
    "        return sentiment_score\n",
    "    \n",
    "    # Metadata\n",
    "    filing_year_quarter = lst[0].replace(\" · Earnings Call Transcript\", \"\").split()\n",
    "    date = lst[1]\n",
    "        \n",
    "    # Store [Position, Caller, Speech, Total Length, Average Segment Length] \n",
    "    position = 0\n",
    "    caller = \"\"\n",
    "    speech = \"\"\n",
    "    paragraph_len_list = []\n",
    "    \n",
    "    # Process segments\n",
    "    data = []\n",
    "    for segment in lst[2:]:\n",
    "        \n",
    "        # Marks new speaker\n",
    "        if len(segment) == 1:\n",
    "            \n",
    "            # Transcript Parsing Error \n",
    "            if segment == \".\":\n",
    "                continue\n",
    "            \n",
    "            # Store current data            \n",
    "            data.append([position, \n",
    "                         caller, \n",
    "                         speech, \n",
    "                         np.sum(paragraph_len_list), \n",
    "                         np.mean(paragraph_len_list), \n",
    "                         get_sentiment(speech)])\n",
    "            \n",
    "            # Reset for next caller\n",
    "            position += 1\n",
    "            caller = \"\"\n",
    "            speech = \"\"\n",
    "            paragraph_len_list = []\n",
    "            \n",
    "            continue\n",
    "            \n",
    "        if caller == \"\":\n",
    "            caller = segment\n",
    "            \n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            speech += segment + \" \"\n",
    "            paragraph_len_list.append(len(segment.split()))\n",
    "            \n",
    "    df = pd.DataFrame(data[1:], columns=[\"position\", \"name\", \"speech\", \"tot_len\", \"avg_len\", \"sentiment\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 100458 - AMD 2022 Q1 - https://roic.ai/transcripts/AMD:US/2022/1\n",
      "The file 'D:\\finance_tools\\transcripts\\AMD_2022_Q1.csv' already exists.\n",
      "1 / 100458 - AMD 2022 Q2 - https://roic.ai/transcripts/AMD:US/2022/2\n",
      "The file 'D:\\finance_tools\\transcripts\\AMD_2022_Q2.csv' already exists.\n",
      "2 / 100458 - AMD 2022 Q3 - https://roic.ai/transcripts/AMD:US/2022/3\n",
      "The file 'D:\\finance_tools\\transcripts\\AMD_2022_Q3.csv' already exists.\n",
      "3 / 100458 - AMD 2022 Q4 - https://roic.ai/transcripts/AMD:US/2022/4\n",
      "The file 'D:\\finance_tools\\transcripts\\AMD_2022_Q4.csv' already exists.\n",
      "4 / 100458 - AMD 2023 Q1 - https://roic.ai/transcripts/AMD:US/2023/1\n",
      "The file 'D:\\finance_tools\\transcripts\\AMD_2023_Q1.csv' already exists.\n",
      "5 / 100458 - ASMIY 2006 Q1 - https://roic.ai/transcripts/ASMIY:US/2006/1\n",
      "6 / 100458 - ASMIY 2006 Q4 - https://roic.ai/transcripts/ASMIY:US/2006/4\n",
      "7 / 100458 - ASMIY 2007 Q1 - https://roic.ai/transcripts/ASMIY:US/2007/1\n",
      "8 / 100458 - ASMIY 2007 Q4 - https://roic.ai/transcripts/ASMIY:US/2007/4\n",
      "9 / 100458 - ASMIY 2009 Q4 - https://roic.ai/transcripts/ASMIY:US/2009/4\n",
      "10 / 100458 - ASMIY 2010 Q1 - https://roic.ai/transcripts/ASMIY:US/2010/1\n",
      "11 / 100458 - ASMIY 2010 Q2 - https://roic.ai/transcripts/ASMIY:US/2010/2\n",
      "12 / 100458 - ASMIY 2010 Q3 - https://roic.ai/transcripts/ASMIY:US/2010/3\n",
      "13 / 100458 - ASMIY 2010 Q4 - https://roic.ai/transcripts/ASMIY:US/2010/4\n",
      "14 / 100458 - ASMIY 2011 Q1 - https://roic.ai/transcripts/ASMIY:US/2011/1\n",
      "15 / 100458 - ASMIY 2011 Q2 - https://roic.ai/transcripts/ASMIY:US/2011/2\n",
      "16 / 100458 - ASMIY 2011 Q3 - https://roic.ai/transcripts/ASMIY:US/2011/3\n",
      "17 / 100458 - ASMIY 2011 Q4 - https://roic.ai/transcripts/ASMIY:US/2011/4\n",
      "18 / 100458 - ASMIY 2012 Q1 - https://roic.ai/transcripts/ASMIY:US/2012/1\n",
      "19 / 100458 - ASMIY 2012 Q2 - https://roic.ai/transcripts/ASMIY:US/2012/2\n",
      "20 / 100458 - ASMIY 2012 Q3 - https://roic.ai/transcripts/ASMIY:US/2012/3\n",
      "21 / 100458 - ASMIY 2012 Q4 - https://roic.ai/transcripts/ASMIY:US/2012/4\n",
      "22 / 100458 - ASMIY 2013 Q1 - https://roic.ai/transcripts/ASMIY:US/2013/1\n",
      "23 / 100458 - ASMIY 2013 Q2 - https://roic.ai/transcripts/ASMIY:US/2013/2\n",
      "24 / 100458 - ASMIY 2013 Q3 - https://roic.ai/transcripts/ASMIY:US/2013/3\n",
      "25 / 100458 - ASMIY 2013 Q4 - https://roic.ai/transcripts/ASMIY:US/2013/4\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(LOG_FILE):\n",
    "    log = log_df.values.tolist()\n",
    "else:\n",
    "    log = []\n",
    "    \n",
    "for index, row in TICKER_DATA.iterrows():\n",
    "    \n",
    "    ticker = row[\"tic\"]\n",
    "    year = row[\"fyearq\"]\n",
    "    quarter = row[\"fqtr\"]\n",
    "    \n",
    "    url = 'https://roic.ai/transcripts/{}:US/{}/{}'.format(*[ticker, year, quarter])\n",
    "\n",
    "    print(\"{} / {} - {} {} Q{} - {}\".format(*[index, TICKER_DATA.shape[0], ticker, year, quarter, url]))\n",
    "\n",
    "    filename = os.path.join(OUTPUT_FOLDER, \n",
    "                            \"{:}_{:}_Q{:}.csv\".format(*[ticker, year, quarter])\n",
    "                           )\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"The file '{filename}' already exists.\")\n",
    "        log.append([ticker, year, quarter, True])\n",
    "        continue\n",
    "\n",
    "    # Get text from URL\n",
    "    try:\n",
    "        earnings_transcript_text = scrape_transcript(url)\n",
    "    except TimeoutException:\n",
    "        log.append([ticker, year, quarter, False])\n",
    "        continue\n",
    "        \n",
    "    # Convert text into list of lists\n",
    "    earnings_transcript_list = clean_text(earnings_transcript_text)\n",
    "    \n",
    "    if earnings_transcript_list == [\"Nothing to show\", \"We apologize for the inconvenience, but there is no content to display at this time.\"] or earnings_transcript_list == []:\n",
    "        print(\"No data...\")\n",
    "        log.append([ticker, year, quarter, False])\n",
    "        continue\n",
    "    \n",
    "    # NER\n",
    "    people, orgs = parse_text(earnings_transcript_text)\n",
    "\n",
    "    # Convert to dataframe\n",
    "    earnings_transcript = split_text(earnings_transcript_list, year, quarter, people, orgs)\n",
    "\n",
    "    # Save\n",
    "    earnings_transcript.to_csv(filename, encoding='utf-8-sig')\n",
    "    log.append([ticker, year, quarter, True])\n",
    "    \n",
    "    if (index + 1) % 10 == 0:\n",
    "        log_df_new = pd.DataFrame(log, columns = [\"tic\", \"fyearq\", \"fqtr\", \"captured\"]).drop_duplicates()\n",
    "        log_df_new.to_csv(LOG_FILE, index=False)\n",
    "        print(\"Saved log file.\")\n",
    "\n",
    "    time.sleep(random.uniform(10, 40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take advantage of div separation via xpath (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # Set up Selenium WebDriver\n",
    "    driver = webdriver.Chrome()  \n",
    "\n",
    "    # Navigate to the website\n",
    "    driver.get(\"https://roic.ai/transcripts/ABT?y=2022&q=4\")\n",
    "\n",
    "    # Find all <div> elements with the class \"p-3 rounded-lq false\"\n",
    "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"p-3\"))).click()\n",
    "    div_elements = driver.find_elements(\"xpath\", '//*[@id=\"__next\"]/div/main/div[3]/div/div[2]/div/div[2]')\n",
    "\n",
    "    # Scrape the text content of each matching <div> element\n",
    "    for div_element in div_elements:\n",
    "        text = div_element.text\n",
    "        print(text)\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
